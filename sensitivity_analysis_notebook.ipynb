{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Info\n",
    "\n",
    "This notebook is just the sensitivity analysis script in a notebook form so that you could load in a bunch of data from a folder and based on the names in the folder you can filter it out based on what you want your plots to look at.\n",
    "\n",
    "The code section bellow is the code required to process the data frames for a specific folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed_data_frame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 123\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files_in_folder:\n\u001b[1;32m    119\u001b[0m \n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# Importing dataframe\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     file_data_frame \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(Path(folder_with_data \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m file))\n\u001b[0;32m--> 123\u001b[0m     file_data_frame \u001b[38;5;241m=\u001b[39m file_data_frame[\u001b[43mprocessed_data_frame\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcitation Freq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200000\u001b[39m]\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m# Converting signal column to numpy array from string\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     file_data_frame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSignal\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m file_data_frame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSignal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mfloat\u001b[39m, ast\u001b[38;5;241m.\u001b[39mliteral_eval(x))) \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mnotnull(x) \u001b[38;5;28;01melse\u001b[39;00m [])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processed_data_frame' is not defined"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "# Packages/ other python files to import.\n",
    "#########################################\n",
    "\n",
    "import ae_process_algos as aepe\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#########################################\n",
    "# User inputs\n",
    "#########################################\n",
    "\n",
    "folder_with_data = \"data_out\"\n",
    "\n",
    "sample_rate = ((125E6)/32) # The ammount of samples taken in a secound.\n",
    "\n",
    "lower_frequency = 120000\n",
    "higher_frequency = 180000\n",
    "threshold = 0.2\n",
    "roll_off = 50\n",
    "\n",
    "#########################################\n",
    "# Functions To Run\n",
    "#########################################\n",
    "\n",
    "def compute_all_ae_processing_algos(signal,sample_rate,lower_frequency,higher_frequency,threshold,roll_off):\n",
    "    \"\"\"\n",
    "    Computes all the ae processing algos for a specific row.\n",
    "    Meant to be used with the apply statement in pandas.\n",
    "    Row format is the one that measure produces in this git page.\n",
    "\n",
    "    Args:\n",
    "        x: represents the row in the data frame\n",
    "        sample_rate: the sample rate when recording the signal\n",
    "        lower_frequency: the lower frequency that is choosen for band energy calculations\n",
    "        higher_frequency: the higher frequency that is choosen for band energy calculations\n",
    "        threshold: the amplitude where it will start counting above that.\n",
    "        roll_off: percentage for the roll off calculations (0-100)\n",
    "    \n",
    "    Returns:\n",
    "        A list of all the new properties to add to the row.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up to run functions one after another.\n",
    "    spectrum = aepe.singal_to_Spectrum(signal)\n",
    "\n",
    "    return pd.Series({\n",
    "            \"band_energy\": aepe.band_energy(spectrum,sample_rate,lower_frequency,higher_frequency),\n",
    "            \"band_energy_ratio\": aepe.band_energy_ratio(spectrum,sample_rate,lower_frequency,higher_frequency),\n",
    "            \"clearance_factor\": aepe.clearance_factor(signal),\n",
    "            \"counts\": aepe.counts(signal,threshold),\n",
    "            \"crest_factor\": aepe.crest_factor(signal),\n",
    "            \"energy\": aepe.energy(signal),\n",
    "            \"impulse_factor\": aepe.impulse_factor(signal),\n",
    "            \"k_factor\": aepe.k_factor(signal),\n",
    "            \"kurtosis\": aepe.kurtosis(signal),\n",
    "            \"margin_factor\": aepe.margin_factor(signal),\n",
    "            \"peak_amplitude\": aepe.peak_amplitude(signal),\n",
    "            \"rms\": aepe.rms(signal),\n",
    "            \"shape_factor\": aepe.shape_factor(signal),\n",
    "            \"skewness\": aepe.skewness(signal),\n",
    "            \"spectral_centroid\": aepe.spectral_centroid(spectrum,sample_rate),\n",
    "            \"spectral_kurtosis\": aepe.spectral_kurtosis(spectrum,sample_rate),\n",
    "            \"spectral_peak_frequency\": aepe.spectral_peak_frequency(spectrum,sample_rate),\n",
    "            \"spectral_rolloff\": aepe.spectral_rolloff(spectrum,sample_rate,roll_off),\n",
    "            \"spectral_skewness\": aepe.spectral_skewness(spectrum,sample_rate),\n",
    "            \"spectral_variance\": aepe.spectral_variance(spectrum,sample_rate),\n",
    "            \"zero_crossing_rate\": aepe.zero_crossing_rate(signal,sample_rate)\n",
    "            })\n",
    "\n",
    "#########################################\n",
    "# Setup\n",
    "#########################################\n",
    "\n",
    "# Files in folder we are working on.\n",
    "files_in_folder = listdir(folder_with_data)\n",
    "\n",
    "# List the properties that will be calculated.\n",
    "properties_to_calculate = [\n",
    "    \"band_energy\",\n",
    "    \"band_energy_ratio\",\n",
    "    \"clearance_factor\",\n",
    "    \"counts\",\n",
    "    \"crest_factor\",\n",
    "    \"energy\",\n",
    "    \"impulse_factor\",\n",
    "    \"k_factor\",\n",
    "    \"kurtosis\",\n",
    "    \"margin_factor\",\n",
    "    \"peak_amplitude\",\n",
    "    \"rms\",\n",
    "    \"shape_factor\",\n",
    "    \"skewness\",\n",
    "    \"spectral_centroid\",\n",
    "    \"spectral_kurtosis\",\n",
    "    \"spectral_peak_frequency\",\n",
    "    \"spectral_rolloff\",\n",
    "    \"spectral_skewness\",\n",
    "    \"spectral_variance\",\n",
    "    \"zero_crossing_rate\"]\n",
    "\n",
    "# Creating a blank data frame to hold all the processed data means and standard deviations from each file\n",
    "# Will then be used to compare against each other with line plots and see how it all differs\n",
    "normal_mean_processed = pd.DataFrame(\n",
    "    columns=properties_to_calculate, index=files_in_folder)\n",
    "normal_std_processed = pd.DataFrame(\n",
    "    columns=properties_to_calculate, index=files_in_folder)\n",
    "\n",
    "\n",
    "#########################################\n",
    "# Main Function running.\n",
    "#########################################\n",
    "\n",
    "# process signal data and save mean and standard deviation.\n",
    "for file in files_in_folder:\n",
    "\n",
    "    # Importing dataframe\n",
    "    file_data_frame = pd.read_csv(Path(folder_with_data + \"/\" + file))\n",
    "\n",
    "    file_data_frame = file_data_frame[processed_data_frame[\"Excitation Freq\"] == 200000]\n",
    "\n",
    "    # Converting signal column to numpy array from string\n",
    "    file_data_frame['Signal'] = file_data_frame['Signal'].apply(\n",
    "        lambda x: list(map(float, ast.literal_eval(x))) if pd.notnull(x) else [])\n",
    "    file_data_frame['Signal'] = file_data_frame['Signal'].apply(np.array)\n",
    "\n",
    "    # Using the apply statement to calculate all ae properties for each data frame row\n",
    "    # Then adding a column for that property\n",
    "    processed_data_frame = file_data_frame.apply(\n",
    "            lambda row: compute_all_ae_processing_algos(\n",
    "                row[\"Signal\"],\n",
    "                sample_rate,\n",
    "                lower_frequency,\n",
    "                higher_frequency,\n",
    "                threshold,\n",
    "                roll_off\n",
    "                ),axis=1)\n",
    "\n",
    "    print(\"Next file\")\n",
    "    #TODO: Create a function to make a string to run all of the relevent functions that a user defines in a list. Make it create the new columns in the data frame and run the funtions of the values.\n",
    "    #TODO: create a function to execute the ae processing algos that we specify in a list. Make it a function.\n",
    "\n",
    "    # Getting mean and standard deviation of properties and placing in tables.\n",
    "    normal_mean_processed.loc[file] = processed_data_frame.mean(axis=0)\n",
    "    normal_std_processed.loc[file] = processed_data_frame.std(axis=0)\n",
    "\n",
    "# Deleating data frames to free up memory once processing is compleated.\n",
    "del(file_data_frame)\n",
    "del(processed_data_frame)\n",
    "print(normal_mean_processed)\n",
    "print(normal_std_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Above print statements\n",
    "\n",
    "Above you should see a reduced table for the mean and standard deviation. The next file statement just helps to keep track of how many data frames it has processed when it is running.\n",
    "\n",
    "## Bellow code\n",
    "\n",
    "The bellow code combines the mean and standard deviation tables to then use the apply statement on the new data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the standard deviation and mean tables to allow the application of the apply statement\n",
    "import pandas as pd\n",
    "\n",
    "# Adding a prefex to the column headers of mean and std dataframes respectivly\n",
    "normal_mean_processed.columns = [f\"mean_{col}\" for col in normal_mean_processed.columns]\n",
    "normal_std_processed.columns = [f\"std_{col}\" for col in normal_std_processed.columns]\n",
    "combined_normal_mean_std = pd.concat([normal_mean_processed, normal_std_processed], axis=1)\n",
    "\n",
    "print(combined_normal_mean_std)\n",
    "print(combined_normal_mean_std.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Above print statments\n",
    "\n",
    "When the above code block runs you will see a data frame of the combined mean and standard deviation data and the column names in the data frame.\n",
    "\n",
    "## Bellow function\n",
    "\n",
    "The bellow function is a simple way to plot a line onto a plt plot. Needs other code around it to make sure lines do not go onto the same plot if we are trying to avoid that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the apply function to use on the data frame.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "def normal_dis_plot(mean,std,line_name):\n",
    "    \"\"\"\n",
    "    Function to take the mean and standard deviation and plot the normal distrabution\n",
    "    \n",
    "    Args:\n",
    "        mean: normal distrabution mean\n",
    "        std: normal distrabution standard deviation\n",
    "        line_name: name that line will be given\n",
    "\n",
    "    Returns:\n",
    "        plots a line of the normal distrabution\n",
    "\n",
    "    \"\"\"\n",
    "    x_values = np.linspace(mean - 1*std, mean + 1*std, 100)\n",
    "    plt.plot(x_values, stats.norm.pdf(x_values, mean, std), label=line_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter point here change like statement as required\n",
    "\n",
    "The statement bellow is where you can change what orignal data frames that have been processed you actually want to make charts for. ie changing the string in the like=... bit will allow you to only include the files containing that text to be included in the pdf doc that will be made.\n",
    "\n",
    "This allows you to make multiple different pdf pages easier as all the data frames have been processed already and you just have to rerun the filter and pdf making code sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [mean_band_energy, mean_band_energy_ratio, mean_clearance_factor, mean_counts, mean_crest_factor, mean_energy, mean_impulse_factor, mean_k_factor, mean_kurtosis, mean_margin_factor, mean_peak_amplitude, mean_rms, mean_shape_factor, mean_skewness, mean_spectral_centroid, mean_spectral_kurtosis, mean_spectral_peak_frequency, mean_spectral_rolloff, mean_spectral_skewness, mean_spectral_variance, mean_zero_crossing_rate, std_band_energy, std_band_energy_ratio, std_clearance_factor, std_counts, std_crest_factor, std_energy, std_impulse_factor, std_k_factor, std_kurtosis, std_margin_factor, std_peak_amplitude, std_rms, std_shape_factor, std_skewness, std_spectral_centroid, std_spectral_kurtosis, std_spectral_peak_frequency, std_spectral_rolloff, std_spectral_skewness, std_spectral_variance, std_zero_crossing_rate]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filtering of the processed data frame operation\n",
    "filtered_combined_normal_mean_std = combined_normal_mean_std.filter(like=\"Head\",axis=0)\n",
    "print(filtered_combined_normal_mean_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Above code\n",
    "\n",
    "Above you should see a small portion of the filtered row wise data frame you have created. helps to make sure everything has worked correctly.\n",
    "\n",
    "## Bellow code\n",
    "\n",
    "Code bellow makes the pdf document with all the attribute plots. The printing of attribute just helps to keep track of prgress.\n",
    "\n",
    "Please make sure to specify a output folder and file name for the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "band_energy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "band_energy_ratio\n",
      "clearance_factor\n",
      "counts\n",
      "crest_factor\n",
      "energy\n",
      "impulse_factor\n",
      "k_factor\n",
      "kurtosis\n",
      "margin_factor\n",
      "peak_amplitude\n",
      "rms\n",
      "shape_factor\n",
      "skewness\n",
      "spectral_centroid\n",
      "spectral_kurtosis\n",
      "spectral_peak_frequency\n",
      "spectral_rolloff\n",
      "spectral_skewness\n",
      "spectral_variance\n",
      "zero_crossing_rate\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    plt.clf() # clear plot object\\n    plt.plot(Iterations,Signal_Float_Array) # plot line\\n    plt.title(\"Signal over Time for \" + Chart_Name) # give chart title\\n    plt.savefig((Chart_Name + \".pdf\"), format=\\'pdf\\') # Save to PDF\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pdf_output = \"pdf_reports\"\n",
    "pdf_file_name = \"change here\"\n",
    "\n",
    "# The pdf file setup\n",
    "pdf_file_of_charts = PdfPages(Path(pdf_output + \"/\" + pdf_file_name))\n",
    "\n",
    "for attribute in properties_to_calculate:\n",
    "    print(attribute)\n",
    "    # The apply statement on the data frame to produce the chart\n",
    "    filtered_combined_normal_mean_std.apply(lambda row: normal_dis_plot(row[(\"mean_\"+attribute)],row[(\"std_\"+attribute)],row.name), axis=1)\n",
    "    plt.title(attribute + \" normal plots\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.xlabel(attribute)\n",
    "    plt.legend()\n",
    "    plt.savefig(pdf_file_of_charts,format=\"pdf\")\n",
    "    plt.clf()\n",
    "\n",
    "# Close full pdf file.\n",
    "pdf_file_of_charts.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rail_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
